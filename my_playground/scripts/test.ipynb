{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/project')\n",
    "\n",
    "\n",
    "import os\n",
    "import clip\n",
    "import torch.nn as nn\n",
    "from datasets import Action_DATASETS\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import argparse\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from dotmap import DotMap\n",
    "import pprint\n",
    "import numpy\n",
    "from modules.Visual_Prompt import visual_prompt\n",
    "from utils.Augmentation import get_augmentation\n",
    "import torch\n",
    "from utils.Text_Prompt import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCLIP(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(TextCLIP, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, text):\n",
    "        return self.model.encode_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCLIP(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ImageCLIP, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.model.encode_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, val_loader, classes, device, model, fusion_model, config, num_text_aug):\n",
    "    model.eval()\n",
    "    fusion_model.eval()\n",
    "    num = 0\n",
    "    corr_1 = 0\n",
    "    corr_5 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_inputs = classes.to(device)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "        for iii, (image, class_id) in enumerate(tqdm(val_loader)):\n",
    "            image = image.view((-1, config.data.num_segments, 3) + image.size()[-2:])\n",
    "            b, t, c, h, w = image.size()\n",
    "            class_id = class_id.to(device)\n",
    "            image_input = image.to(device).view(-1, c, h, w)\n",
    "            image_features = model.encode_image(image_input).view(b, t, -1)\n",
    "            image_features = fusion_model(image_features)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = (100.0 * image_features @ text_features.T)\n",
    "            similarity = similarity.view(b, num_text_aug, -1).softmax(dim=-1)\n",
    "            similarity = similarity.mean(dim=1, keepdim=False)\n",
    "            values_1, indices_1 = similarity.topk(1, dim=-1)\n",
    "            values_5, indices_5 = similarity.topk(5, dim=-1)\n",
    "            num += b\n",
    "            for i in range(b):\n",
    "                if indices_1[i] == class_id[i]:\n",
    "                    corr_1 += 1\n",
    "                if class_id[i] in indices_5[i]:\n",
    "                    corr_5 += 1\n",
    "    top1 = float(corr_1) / num * 100\n",
    "    top5 = float(corr_5) / num * 100\n",
    "    wandb.log({\"top1\": top1})\n",
    "    wandb.log({\"top5\": top5})\n",
    "    print('Epoch: [{}/{}]: Top1: {}, Top5: {}'.format(epoch, config.solver.epochs, top1, top5))\n",
    "    return top1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_type = 'clip_k400'\n",
    "args_arch = 'ViT-B/16'\n",
    "args_dataset = 'hl'\n",
    "args_log_time = '20240229_154250'\n",
    "\n",
    "# load params\n",
    "args_config= \"/home/regal/devel/ws_cacti/src/hri_cacti_xr/gesture_recognition/gesture_recogition_research/ActionCLIP/configs/k400/k400_zero_shot.yaml\"\n",
    "with open(args_config, 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = os.path.join('./exp', config['network']['type'], config['network']['arch'], config['data']['dataset'],\n",
    "                               args_log_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=config['network']['type'],\n",
    "           name='{}_{}_{}_{}'.format(args_log_time, config['network']['type'], config['network']['arch'],\n",
    "                                     config['data']['dataset']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-' * 80)\n",
    "print(' ' * 20, \"working dir: {}\".format(working_dir))\n",
    "print('-' * 80)\n",
    "print('-' * 80)\n",
    "print(' ' * 30, \"Config\")\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(config)\n",
    "print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert yaml params to dot notation\n",
    "config = DotMap(config)\n",
    "print(config.pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(working_dir).mkdir(parents=True, exist_ok=True)\n",
    "shutil.copy(args_config, working_dir)\n",
    "shutil.copy('test.py', working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# If using GPU then use mixed precision training.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # If using GPU then use mixed precision training.\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, clip_state_dict = clip.load(config.network.arch, device=device, jit=False, tsm=config.network.tsm,\n",
    "                                                T=config.data.num_segments, dropout=config.network.drop_out,\n",
    "                                                emb_dropout=config.network.emb_dropout)  # Must set jit=False for training  ViT-B/32\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_val = get_augmentation(False, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_model = visual_prompt(config.network.sim_header, clip_state_dict, config.data.num_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text = TextCLIP(model)\n",
    "model_image = ImageCLIP(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text = torch.nn.DataParallel(model_text).cuda()\n",
    "model_image = torch.nn.DataParallel(model_image).cuda()\n",
    "fusion_model = torch.nn.DataParallel(fusion_model).cuda()\n",
    "wandb.watch(model)\n",
    "wandb.watch(fusion_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = Action_DATASETS(config.data.val_list, config.data.label_list, num_segments=config.data.num_segments,\n",
    "                    image_tmpl=config.data.image_tmpl,\n",
    "                    transform=transform_val, random_shift=config.random_shift)\n",
    "val_loader = DataLoader(val_data, batch_size=config.data.batch_size, num_workers=config.data.workers, shuffle=False,\n",
    "                        pin_memory=True, drop_last=True)\n",
    "print(config.data.val_list)\n",
    "print(config.data.label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cpu\":\n",
    "    model_text.float()\n",
    "    model_image.float()\n",
    "else:\n",
    "    clip.model.convert_weights(\n",
    "        model_text)  # Actually this line is unnecessary since clip by default already on float16\n",
    "    clip.model.convert_weights(model_image)\n",
    "start_epoch = config.solver.start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.pretrain:\n",
    "    if os.path.isfile(config.pretrain):\n",
    "        print((\"=> loading checkpoint '{}'\".format(config.pretrain)))\n",
    "        checkpoint = torch.load(config.pretrain)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        fusion_model.load_state_dict(checkpoint['fusion_model_state_dict'])\n",
    "        del checkpoint\n",
    "    else:\n",
    "        print((\"=> no checkpoint found at '{}'\".format(config.pretrain)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, num_text_aug, text_dict = text_prompt(val_data)\n",
    "best_prec1 = 0.0\n",
    "print(val_loader)\n",
    "print(classes.size())\n",
    "print(num_text_aug)\n",
    "#prec1 = validate(start_epoch, val_loader, classes, device, model, fusion_model, config, num_text_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hri_cacti_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
